Zara ai

import os
import pyttsx3
import speech_recognition as sr
import pywhatkit
import random
import webbrowser
import wikipedia
import datetime
from plyer import notification
import pyautogui
import mtranslate
import open_ai as ai
import pytesseract
import cv2
import time
from PIL import ImageGrab
import pygetwindow as gw
import re
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO
import base64
# -------------------- CONFIG -------------------- #
# Set your Tesseract path here
pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'

# Default input mode: 'voice' or 'text'
INPUT_MODE = "voice"

# Keep last YouTube search query safe
last_search_query = ""

# -------------------- TTS -------------------- #
def speak(audio: str):
    print("ZARA:", audio)
    engine = pyttsx3.init()
    voices = engine.getProperty('voices')
    # NOTE: voices index might differ per system. Adjust if needed.
    try:
        engine.setProperty('voice', voices[2].id)
    except Exception:
        pass
    engine.setProperty("rate", 170)
    engine.say(audio)
    engine.runAndWait()

# -------------------- INPUT HELPERS -------------------- #
def select_input_mode_once():
    """Ask once at startup which mode to use (text/voice)."""
    global INPUT_MODE
    try:
        choice = input("Input mode [t=text, v=voice] (default=v): ").strip().lower()
        if choice == 't':
            INPUT_MODE = 'text'
            speak("Text mode enabled.")
        else:
            INPUT_MODE = 'voice'
            speak("Voice mode enabled.")
    except Exception:
        INPUT_MODE = 'voice'
        speak("Voice mode enabled by default.")

def get_command_text() -> str:
    try:
        return input("Type your command here: ").strip().lower()
    except (EOFError, KeyboardInterrupt):
        return ""

def get_command_voice() -> str:
    content = ""
    while content == "":
        r = sr.Recognizer()
        with sr.Microphone() as source:
            print("Say something!")
            speak("I am listening")
            r.pause_threshold = 1
            try:
                audio = r.listen(source, phrase_time_limit=4)
                content = r.recognize_google(audio, language='en-IN')
                print("You said:", content)
            except Exception:
                # speak("I didn't catch that, please say again")  # keep silent to avoid loops talking
                pass
    return content.lower()

def get_command() -> str:
    if INPUT_MODE == 'text':
        return get_command_text()
    else:
        return get_command_voice()

# -------------------- OCR CLICK -------------------- #
def click_on_text(target_text: str) -> bool:
    time.sleep(1)
    screenshot = ImageGrab.grab()
    screenshot.save("screen.png")
    image = cv2.imread("screen.png")

    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
    for i, word in enumerate(data["text"]):
        if word.lower() == target_text.lower():
            x = data["left"][i]
            y = data["top"][i]
            w = data["width"][i]
            h = data["height"][i]
            print(f"Clicking on '{target_text}' at ({x + w//2}, {y + h//2})")
            pyautogui.moveTo(x + w // 2, y + h // 2)
            pyautogui.click()
            return True
    print(f"Text '{target_text}' not found.")
    speak(f"Text '{target_text}' not found on screen.")
    return False

# -------------------- CLOSE APPS -------------------- #
def close_application(app_name: str):
    exe_name_map = {
        "chrome": "chrome.exe",
        "firefox": "firefox.exe",
        "edge": "msedge.exe",
        "notepad": "notepad.exe",
        "vlc": "vlc.exe",
        "spotify": "Spotify.exe",
        "whatsapp": "WhatsApp.exe",
        "telegram": "Telegram.exe",
        "discord": "Discord.exe",
        "calculator": "CalculatorApp.exe",
        "word": "WINWORD.EXE",
        "excel": "EXCEL.EXE",
        "vs code": "Code.exe",
        "paint": "mspaint.exe",
        "cmd": "cmd.exe",
        "powershell": "powershell.exe",
        "zoom": "Zoom.exe",
        "pycharm": "pycharm64.exe",
        "obs": "obs64.exe",
        "teams": "Teams.exe",
        "file": "explorer.exe",
        "control panel": "control.exe",
        "setting": "SystemSetting.exe",
    }

    exe_name = exe_name_map.get(app_name.lower(), f"{app_name}.exe")
    speak(f"Trying to close {app_name}...")
    try:
        result = os.system(f"taskkill /f /im {exe_name}")
        if result == 0:
            speak(f"{app_name} has been closed.")
        else:
            speak(f"Unable to close {app_name}. Maybe it is not running or the name is incorrect.")
    except Exception as e:
        speak(f"Error occurred while closing {app_name}")
        print("Error:", e)

GEMINI_API_KEY = ""
client_gemini = genai.Client(api_key=GEMINI_API_KEY)

def ask_gemini(question: str) -> str:
    """Gemini clean short answer"""
    try:
        response = client_gemini.models.generate_content(
            model="gemini-2.5-flash",
            contents=f"Answer in max 150 words without ** or markdown formatting:\n{question}",
            config=types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(thinking_budget=0)
            ),
        )
        answer = response.text or ""
        clean_answer = re.sub(r"\*\*|[`#*_]", "", answer)
        clean_answer = re.sub(r"\s+", " ", clean_answer).strip()
        return clean_answer
    except Exception as e:
        return f"Gemini error: {e}" 
def ask_gemini_image(prompt: str):
    """Generate image from Gemini"""
    try:
        response = client_gemini.models.generate_content(
            model="gemini-2.0-flash-preview-image-generation",
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=['TEXT', 'IMAGE']
            )
        )

        for part in response.candidates[0].content.parts:
            # if getattr(part, "text", None):
                # अगर Gemini कोई text भी भेजता है तो सुनाओ
                #speak(part.text)
            if getattr(part, "inline_data", None):
                try:
                    # Base64 decode करो
                    image = Image.open(BytesIO((part.inline_data.data)))
                    filename = "zara_image.png"
                    image.save(filename)
                    image.show()
                    speak(f"Image saved as {filename}")
                except Exception as e:
                    speak(f"Image decode error: {e}")
        return True
    except Exception as e:
        speak(f"Gemini image error: {e}")
        return False

            
# -------------------- MAIN LOOP -------------------- #
def main_process():
    global last_search_query, INPUT_MODE

    select_input_mode_once()  # Ask at start

    while True:
        request = get_command().lower().strip()
        if not request:
            continue
        # Echo only in text mode to avoid TTS loops
        if INPUT_MODE == 'text':
            speak(request)

        # Runtime switching between modes
        if ("text mode" in request and ("on" in request or "enable" in request or "start" in request)) or "switch to text mode" in request:
            INPUT_MODE = 'text'
            speak("Text mode enabled.")
            continue
        if ("voice mode" in request and ("on" in request or "enable" in request or "start" in request)) or "switch to voice mode" in request:
            INPUT_MODE = 'voice'
            speak("Voice mode enabled.")
            continue

        if "hi" in request:
            speak("Welcome, Alfaaz how can I help you.")

        elif "banaya" in request:
            speak("I have been made by a BCA group which includes Alfaaz, SATYAM, Himanshu, Deepak, Akash")

        elif "whatsapp" in request and "bolo" in request:
            match = re.search(r"(.*?)\s*ko\s*whatsapp\s*par\s*bolo\s*(.+)", request)
            if match:
                contact = match.group(1).strip()
                message = match.group(2).strip()
                speak(f"{contact} ko WhatsApp par ye sandesh bhej raha hoon: {message}")

                webbrowser.open("https://web.whatsapp.com")
                time.sleep(25)

                try:
                    pyautogui.click(150, 200)  # Adjust this as needed
                    time.sleep(2)

                    pyautogui.write(contact)
                    time.sleep(3)
                    pyautogui.press("enter")
                    time.sleep(2)

                    pyautogui.write(message)
                    time.sleep(1)
                    pyautogui.press("enter")

                    print("Message sent - about to speak")
                    speak("Message bhej diya gaya hai.")
                except Exception as e:
                    print("ERROR:", e)
                    speak("Message bhejne mein samasya aayi.")
            else:
                speak("WhatsApp command samajh nahi aaya. Kripya dobara try karein.")

        elif "play music" in request:
            speak("Playing music")
            song = random.randint(1, 3)
            webbrowser.open("https://www.youtube.com/watch?v=8of5w7RgcTc&list=RD8of5w7RgcTc&start_radio=1")

        elif "open youtube" in request:
            speak("Opening YouTube")
            webbrowser.open("https://www.youtube.com")

        elif "search" in request and "youtube" in request:
            last_search_query = request.replace("search", "").replace("on youtube", "").strip()
            speak(f"Searching {last_search_query} on YouTube")
            webbrowser.open(f"https://www.youtube.com/results?search_query={last_search_query}")
            speak("Here are the results. Say 'play first video' to start.")

        elif "play first video" in request and last_search_query:
            speak(f"Playing {last_search_query}")
            pywhatkit.playonyt(last_search_query)

        elif "stop" in request or "play" in request:
            pyautogui.press("k")
            speak("Toggled play and pause")

        elif "full screen" in request:
            pyautogui.press("f")
            speak("Fullscreen enabled")

        elif "half screen" in request:
            pyautogui.press("f")
            speak("halfscreen enabled")

        elif "mute" in request:
            pyautogui.press("m")
            speak("Mute toggled")

        elif "change" in request:
            pyautogui.hotkey("shift", "n")
            speak("Next video playing")

        elif "open instagram" in request:
            speak("Opening Instagram")
            song = random.randint(1, 3)
            webbrowser.open("https://www.instagram.com")

        elif "time" in request:
            now_time = datetime.datetime.now().strftime("%H:%M")
            speak("Current time is " + str(now_time))

        elif "date" in request:
            now_date = datetime.datetime.now().strftime("%d:%m")
            speak("Current date is " + str(now_date))

        elif "new work" in request:
            task = request.replace("new task", "").strip()
            if task != "":
                speak("Adding task: " + task)
                with open("zara.txt", "a", encoding='utf-8') as file:
                    file.write(task + "\n")

        elif "speak task" in request:
            try:
                with open("zara.txt", "r", encoding='utf-8') as file:
                    speak("Work we have to do today is " + file.read())
            except FileNotFoundError:
                speak("No tasks found yet.")

        elif "so work" in request:
            try:
                with open("zara.txt", "r", encoding='utf-8') as file:
                    task = file.read()
                    notification.notify(
                        title="Today's Work",
                        message=task
                    )
            except FileNotFoundError:
                speak("No tasks found to show.")

        elif "open" in request:
            query = request.replace("open", "").strip()
            pyautogui.press("super")
            pyautogui.typewrite(query)
            pyautogui.sleep(2)
            pyautogui.press("enter")

        elif "wikipedia" in request:
            q = request.replace("search wikipedia", "").replace("wikipedia", "").strip()
            if q:
                try:
                    result = wikipedia.summary(q, sentences=2)
                    print(result)
                    speak(result)
                except Exception:
                    speak("Couldn't fetch from Wikipedia. Try another query.")
            else:
                speak("Please tell me what to search on Wikipedia.")

        elif request.startswith("search "):
            q = request.replace("search", "").strip()
            if q:
                webbrowser.open("https://www.google.com/search?q=" + q)
                speak("Here are the Google results")
            else:
                speak("Please tell me what to search.")
        elif request.startswith("zara"):
            query = request.replace("gemini", "").strip()
            if query:
                speak("Alfaz pleas wait...")
                answer = ask_gemini(query)
                speak(answer)
            else:
                speak("Please Zara ke liye koi sawal pucho.")  
            continue
        elif request.startswith("image"):
           query = request.replace("image", "").strip()
           if query:
              speak("Image generate kar rahi hoon Alfaz...")
              ask_gemini_image(query)
           else:
              speak("Please image ke liye koi prompt batao.")
           continue


        elif "?" in request:
            # This block seems to call your custom OpenAI wrapper
            try:
                response = ai.send(request)
                speak(response)
            except Exception as e:
                print("AI ERROR:", e)
                speak("AI response me problem aayi.")

        elif "screenshot" in request:
            if "center" in request:
                region = (600, 300, 700, 400)  # (x, y, width, height)
                area = "center"
            elif "left" in request or "corner" in request:
                region = (0, 0, 500, 400)
                area = "top left corner"
            elif "right" in request:
                region = (1400, 0, 500, 400)
                area = "top right corner"
            else:
                region = (0, 0, 1920, 1080)  # default region
                area = "default region"

            now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f"{area.replace(' ', '_')}_screenshot_{now}.png"

            screenshot = pyautogui.screenshot(region=region)
            screenshot.save(filename)
            speak(f"{area} screenshot saved as {filename}")

        elif "click" in request:
            button_name = request.replace("click", "").strip().lower()
            if button_name:
                speak(f"Trying to click on {button_name}")
                result = click_on_text(button_name)
                if result:
                    speak(f"Clicked on {button_name}")
                else:
                    speak(f"Could not find any button named {button_name}")
            else:
                speak("Please tell me which text to click on.")

        elif "close" in request:
            app_name = request.replace("close", "").strip()
            if app_name != "":
                close_application(app_name)
            else:
                speak("Please specify an app to close.")

        elif request in ["quit", "exit", "bye", "jarvish band karo", "stop jarvish"]:
            speak("Jarvish band ho raha hai. Bye!")
            break

        else:
            speak("Yeh command mujhe nahi aayi")


if __name__ == "__main__":
    main_process()








import google.generativeai as genai

# API Key configure karo
genai.configure(api_key="")

# Model instance banao
model = genai.GenerativeModel("gemini-1.5-flash")

def send(query):
    response = model.generate_content(query)
    return response.text
